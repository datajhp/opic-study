import streamlit as st
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode
import whisper
import torchaudio
import torch
import tempfile
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import av
import numpy as np
import queue

# Whisper + grammar model ë¡œë”©
@st.cache_resource
def load_whisper():
    return whisper.load_model("base")

@st.cache_resource
def load_grammar_model():
    tokenizer = AutoTokenizer.from_pretrained("prithivida/grammar_error_correcter_v1")
    model = AutoModelForSeq2SeqLM.from_pretrained("prithivida/grammar_error_correcter_v1")
    return tokenizer, model

st.title("ğŸ™ ë§ˆì´í¬ë¡œ ì˜¤í”½ ì‘ë‹µ ë…¹ìŒ + í”¼ë“œë°± ì‹œìŠ¤í…œ")

# ì˜¤ë””ì˜¤ ë²„í¼ìš© í ìƒì„±
audio_queue = queue.Queue()

# ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤ ì •ì˜
class AudioProcessor(AudioProcessorBase):
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        # Float32ë¡œ ë³€í™˜
        pcm = frame.to_ndarray().flatten().astype(np.float32)
        audio_queue.put(pcm)
        return frame

# ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
webrtc_ctx = webrtc_streamer(
    key="mic",
    mode=WebRtcMode.SENDONLY,  # âœ… ì˜¬ë°”ë¥¸ enum íƒ€ì…ìœ¼ë¡œ ì„¤ì •
    audio_receiver_size=1024,
    media_stream_constraints={"audio": True, "video": False},
    audio_processor_factory=AudioProcessor,
    async_processing=True,
)

# ë…¹ìŒì´ ì™„ë£Œë˜ë©´ ë²„íŠ¼ í´ë¦­
if st.button("âœ… ë…¹ìŒ ì¢…ë£Œ í›„ í”¼ë“œë°± ë°›ê¸°"):
    st.info("ë…¹ìŒëœ ìŒì„±ì„ ì €ì¥í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")

    # ì˜¤ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    all_audio = []
    while not audio_queue.empty():
        all_audio.extend(audio_queue.get())

    audio_tensor = torch.tensor(all_audio)
    sample_rate = 48000  # streamlit_webrtc ê¸°ë³¸ ìƒ˜í”Œë§

    # ì„ì‹œ wav íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        torchaudio.save(f.name, audio_tensor.unsqueeze(0), sample_rate)
        audio_path = f.name

    # Whisperë¡œ í…ìŠ¤íŠ¸ ì „í™˜
    whisper_model = load_whisper()
    result = whisper_model.transcribe(audio_path)
    transcript = result["text"]

    st.subheader("ğŸ“ ì „ì‚¬ëœ í…ìŠ¤íŠ¸")
    st.write(transcript)

    # ë¬¸ë²• êµì •
    tokenizer, grammar_model = load_grammar_model()
    input_ids = tokenizer.encode("gec: " + transcript, return_tensors="pt", max_length=512, truncation=True)
    outputs = grammar_model.generate(input_ids, max_length=512, num_beams=4)
    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)

    st.subheader("âœ… êµì • ê²°ê³¼")
    st.markdown(f"**ìˆ˜ì •ëœ ë¬¸ì¥:** {corrected}")

