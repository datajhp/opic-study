import streamlit as st
import whisper
import tempfile
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode
import av
import queue

# ì „ì—­ ë³€ìˆ˜ë¡œ ë…¹ìŒ ìƒíƒœë¥¼ ê´€ë¦¬
st.session_state.setdefault("recording", False)

# ì˜¤ë””ì˜¤ ë²„í¼ í
audio_queue = queue.Queue()

# Audio ì²˜ë¦¬ í´ë˜ìŠ¤
class AudioProcessor(AudioProcessorBase):
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        pcm = frame.to_ndarray().flatten().astype(np.float32)
        audio_queue.put(pcm)
        return frame

# ëª¨ë¸ ë¡œë”©
@st.cache_resource
def load_whisper():
    return whisper.load_model("base")

@st.cache_resource
def load_grammar_model():
    tokenizer = AutoTokenizer.from_pretrained("vennify/t5-base-grammar-correction")
    model = AutoModelForSeq2SeqLM.from_pretrained("vennify/t5-base-grammar-correction")
    return tokenizer, model

def grammar_correction(text):
    tokenizer, model = load_grammar_model()
    inputs = tokenizer.encode(text, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(inputs, max_length=512, num_beams=4)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

st.title("ğŸ™ ì˜¤í”½ ë§í•˜ê¸° ë…¹ìŒ ì—°ìŠµ")

if "recording" not in st.session_state:
    st.session_state.recording = False
# 1ë‹¨ê³„: ë…¹ìŒ ì‹œì‘
if not st.session_state.recording:
    if st.button("ğŸ¤ ë…¹ìŒ ì‹œì‘"):
        st.session_state.recording = True
        # âŒ st.experimental_rerun() ì œê±°
if st.session_state.recording:
    st.success("ğŸ”´ ë…¹ìŒ ì¤‘ì…ë‹ˆë‹¤! ë§í•˜ê³  ë‚˜ì„œ ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

    webrtc_ctx = webrtc_streamer(
        key="mic-recorder",
        mode=WebRtcMode.SENDONLY,
        audio_receiver_size=256,
        media_stream_constraints={"audio": True, "video": False},
        audio_processor_factory=AudioProcessor,
        async_processing=True,
    )

    if webrtc_ctx and webrtc_ctx.state.playing:
        st.info("ğŸ§ ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ë§í•´ë³´ì„¸ìš”!")
    elif webrtc_ctx and not webrtc_ctx.state.playing:
        st.warning("â³ ë§ˆì´í¬ ì—°ê²° ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤...")
    else:
        st.warning("ğŸ›‘ ë§ˆì´í¬ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤...")

    # 2ë‹¨ê³„: ë…¹ìŒ ì¢…ë£Œ + ë¶„ì„
    if st.button("âœ… ë…¹ìŒ ì¢…ë£Œ ë° ë¶„ì„"):
        st.session_state.recording = False

        if not audio_queue.empty():
            all_audio = []
            while not audio_queue.empty():
                all_audio.extend(audio_queue.get())
            audio_tensor = torch.tensor(all_audio)
            sample_rate = 48000

            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                tmp_audio_path = f.name
                import torchaudio
                torchaudio.save(tmp_audio_path, audio_tensor.unsqueeze(0), sample_rate)

            whisper_model = load_whisper()
            result = whisper_model.transcribe(tmp_audio_path)
            user_text = result["text"]
            st.subheader("ğŸ“ ì „ì‚¬ëœ í…ìŠ¤íŠ¸")
            st.write(user_text)

            corrected = grammar_correction(user_text)
            st.subheader("âœ… êµì •ëœ ë¬¸ì¥")
            st.success(corrected)
        else:
            st.error("ğŸ™ ë…¹ìŒëœ ìŒì„±ì´ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ ê¶Œí•œ ë˜ëŠ” ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
